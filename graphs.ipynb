{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "%pip install seaborn\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n\ndata = pd.read_csv('colorado90.csv')\n\n# Check for missing values\nif data.isnull().sum().any():\n    #none of these work in jupyter\n    #data = data.ffill()\n    #data = data.fillna(data.mean())\n    #data = data.fillna(data.mode().iloc[0])\n    data = data.fillna(0)\n\n# Split data into features (X) and target (y)\nX = data.drop('KWH_TOTAL', axis=1)\ny = data['KWH_TOTAL']\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 77)\n\n# Using RandomForestRegressor model\nmodel = RandomForestRegressor(n_estimators=100, random_state = 77)\n\n# Load training data\nmodel.fit(X_train, y_train)\n\n# Get the feature importance values\nimportances = model.feature_importances_\n\n# Use model to make predictions on the test data\ny_pred = model.predict(X_test)\n\n# Calculate mean absolute error\nmae = mean_absolute_error(y_test, y_pred)\n\n# Calculate the R-squared score of the model\nr2 = r2_score(y_test, y_pred)\n\nprint(\"Mean Absolute Error (MAE): \", mae)\nprint(\"R-squared (R2 score): \", r2)\n\n#sort features by importance, in descending order\nfeatures = X.columns\nimportances = model.feature_importances_\nindices = np.argsort(importances)[::-1]\n\n#print top 13 features in order of importance\nprint(\"Top 13 features:\")\nfor f in range(13):\n    print(f\"{f + 1}. {features[indices[f]]}: {importances[indices[f]]}\")\n\n\nimport matplotlib.pyplot as plt\n\n# histogram\n\nplt.hist(data['KWH_TOTAL'], bins=30, edgecolor='black')\nplt.title('Histogram of KWH_TOTAL')\nplt.xlabel('KWH_TOTAL')\nplt.ylabel('Frequency')\n#plt.show()\nplt.savefig('histogram.png')\n\n\n#regression line\n\n# Use the model to make predictions on the test data\ny_pred = model.predict(X_test)\n# Create a scatter plot of the actual vs predicted values\nplt.scatter(y_test, y_pred, color='blue')\n# Calculate the line of best fit\nm, b = np.polyfit(y_test, y_pred, 1)\n# Add the line of best fit to the plot\nplt.plot(y_test, m*y_test + b, color='red')\n\nplt.title('Actual vs Predicted Values for KWH_TOTAL')\nplt.xlabel('Actual KWH')\nplt.ylabel('Predicted KWH')\nplt.savefig('regressionLine.png')\n#plt.show()\n\n# correlation matrix\nimport seaborn as sns\n\n# Get the top 13 features\ntop_features = [features[i] for i in indices[:13]]\n# Select the top 10 features from the data\ndata_top_features = data[top_features]\n# Calculate the correlation matrix\ncorr_matrix_top = data_top_features.corr()\n# Create a heatmap of the correlation matrix\nsns.heatmap(corr_matrix_top, annot=True, fmt=\".2f\")\n# Display the plot\nplt.savefig('correlationMatrix.png')  # Save the figure before showing it\n#plt.show()",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Mean Absolute Error (MAE):  2014.8880021276595\nR-squared (R2 score):  0.4691731297352053\nTop 13 features:\n1. SQFTEST: 0.11934650961193075\n2. TVCOLOR: 0.09925066336394758\n3. NUMSMPHONE: 0.06927957839486358\n4. DWASHUSE: 0.06289255814542863\n5. PLAYSTA: 0.029348318074539918\n6. INTSTREAM: 0.027212879744704148\n7. OVEN: 0.02715281951938421\n8. NUMFREEZ: 0.01883030216801456\n9. CABLESAT: 0.01710921499142525\n10. NHSLDMEM: 0.016720189553098473\n11. LGTOUTNITE: 0.01661842668011275\n12. TOTROOMS: 0.013442475110360886\n13. BEDROOMS: 0.013295707849708965\n",
          "output_type": "stream"
        }
      ],
      "execution_count": 5
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}